# Day 1 - intro to unix/linux coding

### Intro to bioinformatics workflows

My bioinformatics workflows usually begin on a unix/linux command line interface (CLI) and end in R studio. The CLI takes advantage of 
OSU's computing infrastructure to handle big data processing using memory intensive programs. The final output from these programs is in 
a smaller, more interpretable format that can be imported into R studio for data analysis and visualizations.

For example, my RNAseq workflow begins with the RNA reads generated by an Illumina NextSeq, which I begin processing on the CLI. 

[Here](https://www.youtube.com/watch?v=fCd6B5HRaZ8) is a nice short video about how Illumina sequencing works if you would like a refresher. The important takeaways for data processing are that reads of a predetermined consistent length are generated be the sequencing machine. These reads can either be [single end](https://www.illumina.com/science/technology/next-generation-sequencing/plan-experiments/paired-end-vs-single-read.html) (one sequence in one direction is generated per DNA or RNA fragment) or paired end (two sequences covering the forward and reverse direction of each read are generated). The RNA data we are working with is paired end. Using paired end data improves that read mapping step, as it's easier to correctly map a read if it has a second, complimentary read to map at the other end of the fragment.

The RNAseq workflow I use begins with those paired-end reads, then filters them to remove adapters, poly-A tails, low quality bases, and 
ribosomal RNA (rRNA). When the sequencing machine does it's sequencing, it also assigns a quality score to each base call that represents how confident the call is. We can set a certain quality threshold that tells a program what reads to keep, and what reads to filter out. 

After filtering, the reads are mapped to the reference genome and then sorted. The last CLI data processing task is then to use a program to count how many reads are mapping to each gene. The output from this is a counts matrix, where the columns are the samples in the experiment and the rows are the genes. This way, we can compare the relative expression (number of counts) of each gene between samples. 

This project will be more targetted than a standard RNAseq experiment, in which we would look at the whole transcriptome. We will focus 
our analysis on NLR genes (nucleotide-binding leucine-rich repeats) which are a group of genes related to plant immunity that are highly 
conserved across the plant kingdom.

### Intro to the command line and OSU's infrastructure

Most of the coding you'll do on the command line will use bash scripts. Bash is a shell program, which is a command line interpreter. You'll use the command line in two different ways: you will directly type commands on the command line, and you will use the command line to submit scripts to a node, or processing machine. OSU has dozens of nodes, and several are assigned to BPP (the dept of botany and plant pathology). 

You should have an account on the CGRB (center for genome research and biology at OSU) cluster. To log in, you will need a program on your computer. I use putty, which you can download [here] (LINK). When you open putty, input the host name shell.cgrb.oregonstate.edu, and the port number that was provided when you made an account. The program will open and you will be prompted to enter the username given to you by cgrb, then your password and 2-factor authentication. 

Then the command line will open! 

Every time you log in to the cluster, you will begin on the machine called vaughn. This is a front-end machine and is not intended for processing jobs. If you run a job on vaughn, there is a memory limit and your job will automatically be killed if it exceeds a certain memory threshold. To run processing commands, we either need to submit a script. This will run in the background on a separate machine and won't occupy the command line, so you can continue working while it runs. Or, we use the command qrsh to log on to a processing machine. You can try that - 

`qrsh`

You will see the name of the machine you are logged into on the command line. To get back on vaughn, type - 

`exit`

If you type `exit` on vaughn you will be logged out of the cluster. 

[Here] (LINK) is an overview of the CGRB infrastructure. I think it's worth looking over now, but we will keep coming back to this info so there is no need to memorize anything at this point. 

### Navigating on the command line

You can't run processing jobs on vaughn, but you can navigate around your directories. To find out where you are, type

`pwd`

which means 'print working directory.'

To print everything that's in your current directory, type

`ls`

There should be nothing. To make a directory, the command is `mkdir`. For an example, let's make a directory we will need for future analyses:

`mkdir fastp_out`

[fastp](https://github.com/OpenGene/fastp) is the program we will use for basic read filtering. To move into this directory, use

`cd fastp_out`

To move back up one directory:

`cd ..`

On the command line, . refers to the current directory, and .. moves you up one directory. To move up two, use ../..

From your main directory, this command will show every file upstream. 

`tree`

A useful trick that will speed up your navigation and prevent you from getting lost by typos is tab-completion. You should be able to move into the fastp_out dir by typing `cd f` and hitting tab. 

If you use `ls` now, you should be able to see the directory you made. 

### Running programs

The cluster has a ton of software already downloaded onto it. Everything you will use is either already on the cluster, or I've downloaded it. All of the cluster's software for general use is stored in a directory. To see where, use the command `which`. This will tell you where fastp is downloaded - 

`which fastp`

These programs are set up so that you can run them just by typing their name, without specifying that full path. Many programs will display their options if you just type the name and nothing else. To see some basic info and parameters you can set for fastp, type

`fastp` 

or

`fastp --help`

to check what software version we are using, type

`fastp --version`

It's important to keep track of which software options you use, since updated versions can have different options, or the same parameters could have a new function. Keeping track of versions allows your research to be reproducible. 

###########################################################################################################################

### Sudden Oak Death

Changing gears now. Here is a quick intro to Sudden Oak Death. The RNAseq data we will analyze comes from tanoak (Notholithocarpus densiflorus) infected with Phytophthora ramorum, which is an oomycete. 

[Here] is an overview of Phytophthoras (aka the plant destroyers). [Here] is a cool website going over morphology and other information on some important phytophthoras that infect forest trees. 

And [here] is the Oregon Sudden Oak Death dashboard. This webpage is maintained by the interagency SOD program, including the forest service, oregon department of forestry, osu, other others. You can see a map of the epidemic, including quarantine zones, individual infected trees we've sampled from, which lineage of P. ramorum was detected, etc. When the epidemic began, eradication was the goal. You can see on the map that there is now a "generally infested area." Eradication did not work. 

And here are a few more SOD resources to look over. 
